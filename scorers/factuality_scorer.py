from pydantic import BaseModel

from lmeval.enums import Modality, ScorerType
from lmeval.models.lmmodel import LMAnswer
from lmeval.question import Question
from lmeval.scorers.scorer import Scorer


class FactualityResponse(BaseModel):
    correct: bool
    reason: str


class FactualityScorer(Scorer):
    name: str = "factuality_scorer"
    description: str = "Return 1.0 if the model answer is aligned with the reference answer, 0.0 otherwise"
    type: ScorerType | str = "factuality"
    modality: Modality = Modality.text

    # Use a string match to check if the reference answer is in the generated answer.
    # - If there is a match, the answer is CORRECT (i.e. the model answer contains the fact).
    # - If no string match: use an LLM to check if the generated answer is factually correct according to the reference.

    system_prompt: str = """You are a powerful AI auditor, your task is to evaluate the question and answer generated by the AI model.
    You will be provided with a question and an answer generated by the model and you must evaluate the answer with respect to the ground truth.
    Return your answer as a json object with the key "correct" (boolean) and "reason" (string) to explain your decision. Here is an example of the expected output:
    
    <question>What is the capital of France?</question>
    <ground_truth>Paris</ground_truth>
    <model_answer>London</model_answer>
    {"correct": false, "reason": "The model answer is incorrect because it does not align with the ground truth answer Paris."}
    """

    evaluation_prompt: str = """
    Here is the question and the corresponding ground truth:
    <question>{question}</question>
    <ground_truth>{ground_truth}</ground_truth>

    Here is the answer you need to evaluate:
    <model_answer>{model_answer}</model_answer>
    """

    debug: bool = False

    def score(
        self,
        model_answer: LMAnswer,
        question: Question,
        task,
    ) -> float:

        messages = [{"role": "system", "content": self.system_prompt}]
        messages.append(
            {
                "role": "user",
                "content": self.evaluation_prompt.format(
                    question=question.metadata["question"],
                    ground_truth=question.metadata["reference_answer"],
                    model_answer=model_answer.answer,
                ),
            }
        )

        vote = self.model.majority_vote(
            messages, decision_key="correct", response_format=FactualityResponse
        )
        if self.debug:
            print("--------------------------------")
            print("Question: ", question.metadata["question"])
            print("Reference answer: ", question.metadata["reference_answer"])
            print(f"{model_answer.model.name} answer: ", model_answer.answer)
            print("Vote", vote)
            print("--------------------------------")

        model_answer.score_raw_data[self.type] = vote.raw_responses
        return 1.0 if vote.decision else 0.0
